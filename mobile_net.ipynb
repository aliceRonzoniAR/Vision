{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dSfeOSXelk3"
      },
      "outputs": [],
      "source": [
        "USE_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoS5FmCaemkL"
      },
      "outputs": [],
      "source": [
        "curren_dir = '/content/drive/MyDrive/path/to/folder/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y5lqleXeogg"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if USE_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    sys.path.append(curren_dir) ## Place correct\n",
        "else:\n",
        "    sys.path.append('')\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import glob as glob\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsV_VjOew8g"
      },
      "outputs": [],
      "source": [
        "import matplotlib.patches as patches\n",
        "\n",
        "def plot_image(img, boxes, scores, labels, dataset, save_path=None):\n",
        "\n",
        "  '''\n",
        "  Function that draws the BBoxes, scores, and labels on the image.\n",
        "\n",
        "  inputs:\n",
        "    img: input-image as numpy.array (shape: [H, W, C])\n",
        "    boxes: list of bounding boxes (Format [N, 4] => N times [xmin, ymin, xmax, ymax])\n",
        "    scores: list of conf-scores (Format [N] => N times confidence-score between 0 and 1)\n",
        "    labels: list of class-prediction (Format [N] => N times an number between 0 and _num_classes-1)\n",
        "    dataset: list of all classes e.g. [\"background\", \"class1\", \"class2\", ..., \"classN\"] => Format [N_classes]\n",
        "  '''\n",
        "\n",
        "  cmap = plt.get_cmap(\"tab20b\")\n",
        "  class_labels = np.array(dataset)\n",
        "  colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n",
        "  height, width, _ = img.shape\n",
        "  # Create figure and axes\n",
        "  fig, ax = plt.subplots(1, figsize=(16, 8))\n",
        "  # Display the image\n",
        "  ax.imshow(img)\n",
        "\n",
        "  for i, box in enumerate(boxes):\n",
        "    class_pred = labels[i]\n",
        "    conf = scores[i]\n",
        "    width = box[2] - box[0]\n",
        "    height = box[3] - box[1]\n",
        "    rect = patches.Rectangle(\n",
        "        (box[0], box[1]),\n",
        "        width,\n",
        "        height,\n",
        "        linewidth=2,\n",
        "        edgecolor=colors[int(class_pred)],\n",
        "        facecolor=\"none\",\n",
        "    )\n",
        "    # Add the patch to the Axes\n",
        "    ax.add_patch(rect)\n",
        "    plt.text(\n",
        "        box[0], box[1],\n",
        "        s=class_labels[int(class_pred)] + \" \" + str(int(100*conf)) + \"%\",\n",
        "        color=\"white\",\n",
        "        verticalalignment=\"top\",\n",
        "        bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
        "    )\n",
        "\n",
        "  # Used to save inference phase results\n",
        "  if save_path is not None:\n",
        "    plt.savefig(save_path)\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utMp_n6ZClzV"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_MobileNet_V3_Large_FPN_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsDJeb8teqd6"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(\"We use the following device: \", device)\n",
        "# load a model; pre-trained on COCO\n",
        "# model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights='COCO_V1').to(device)\n",
        "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=\"COCO_V1\").to(device)\n",
        "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r6mOcGDrnGy"
      },
      "outputs": [],
      "source": [
        "def img_transform(img):\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "  img /= 255.0\n",
        "  img = torch.from_numpy(img).permute(2,0,1)\n",
        "  return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyfGv2hQe26O"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYP2-JzcezBX"
      },
      "outputs": [],
      "source": [
        "# # Import dataset\n",
        "\n",
        "# !pip install roboflow\n",
        "\n",
        "# from roboflow import Roboflow\n",
        "# rf = Roboflow(api_key=\"hHft0q8E9j8NLCGwL0rE\")\n",
        "# project = rf.workspace(\"nora-slimani\").project(\"trash-detection-otdmj\")\n",
        "# dataset = project.version(35).download(\"coco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3W3iKphiBpQ"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"hHft0q8E9j8NLCGwL0rE\")\n",
        "project = rf.workspace(\"trash-dataset-for-oriented-bounded-box\").project(\"trash-detection-1fjjc\")\n",
        "version = project.version(14)\n",
        "dataset = version.download(\"coco\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEtcbY-JfDbf"
      },
      "outputs": [],
      "source": [
        "# importare il file 'train_images_info.csv'\n",
        "train_df = pd.read_csv(r'/content/train_images_info.csv')\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeBlmoHFgENA"
      },
      "outputs": [],
      "source": [
        "train_df[\"x1\"] = pd.to_numeric(train_df[\"x1\"])\n",
        "train_df[\"y1\"] = pd.to_numeric(train_df[\"y1\"])\n",
        "train_df[\"w\"] = pd.to_numeric(train_df[\"w\"])\n",
        "train_df[\"h\"] = pd.to_numeric(train_df[\"h\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jN0GRkXgGLT"
      },
      "outputs": [],
      "source": [
        "train_df[\"x2\"] = train_df[\"x1\"] + train_df[\"w\"]\n",
        "train_df[\"y2\"] = train_df[\"y1\"] + train_df[\"h\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGVpEO2lgIWW"
      },
      "outputs": [],
      "source": [
        "train_df.drop(columns=['w', 'h', 'height', 'width'], inplace = True)\n",
        "train_df = train_df[['ann_id', 'image_id', 'category_id', 'x1', 'y1', 'x2', 'y2', 'file_name']]\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ApnxUVxU6QX"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_Og8yNtU8Kt"
      },
      "outputs": [],
      "source": [
        "# importare il file 'train_images_info.csv'\n",
        "valid_df = pd.read_csv(r'/content/valid_images_info.csv')\n",
        "valid_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDmg01a8VVsD"
      },
      "outputs": [],
      "source": [
        "valid_df[\"x1\"] = pd.to_numeric(valid_df[\"x1\"])\n",
        "valid_df[\"y1\"] = pd.to_numeric(valid_df[\"y1\"])\n",
        "valid_df[\"w\"] = pd.to_numeric(valid_df[\"w\"])\n",
        "valid_df[\"h\"] = pd.to_numeric(valid_df[\"h\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W4bKk1dVYlQ"
      },
      "outputs": [],
      "source": [
        "valid_df[\"x2\"] = valid_df[\"x1\"] + valid_df[\"w\"]\n",
        "valid_df[\"y2\"] = valid_df[\"y1\"] + valid_df[\"h\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMiXPGRbVaDX"
      },
      "outputs": [],
      "source": [
        "valid_df.drop(columns=['w', 'h', 'height', 'width'], inplace = True)\n",
        "valid_df = valid_df[['ann_id', 'image_id', 'category_id', 'x1', 'y1', 'x2', 'y2', 'file_name']]\n",
        "valid_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haXspVhJir-R"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXYiehiqiqos"
      },
      "outputs": [],
      "source": [
        "# importare il file 'train_images_info.csv'\n",
        "test_df = pd.read_csv(r'/content/test_images_info.csv')\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE7fjtThizAh"
      },
      "outputs": [],
      "source": [
        "test_df[\"x1\"] = pd.to_numeric(test_df[\"x1\"])\n",
        "test_df[\"y1\"] = pd.to_numeric(test_df[\"y1\"])\n",
        "test_df[\"w\"] = pd.to_numeric(test_df[\"w\"])\n",
        "test_df[\"h\"] = pd.to_numeric(test_df[\"h\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M14dNhZPi7xc"
      },
      "outputs": [],
      "source": [
        "test_df[\"x2\"] = test_df[\"x1\"] + test_df[\"w\"]\n",
        "test_df[\"y2\"] = test_df[\"y1\"] + test_df[\"h\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWCQ-EYgjDA3"
      },
      "outputs": [],
      "source": [
        "test_df.drop(columns=['w', 'h', 'height', 'width'], inplace = True)\n",
        "test_df = test_df[['ann_id', 'image_id', 'category_id', 'x1', 'y1', 'x2', 'y2', 'file_name']]\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU4ojxq-VBTj"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3EBIIlHgT7e"
      },
      "outputs": [],
      "source": [
        "# scrivo train_df in file train_df.csv\n",
        "train_df.to_csv(\"/content/train_df.csv\")\n",
        "valid_df.to_csv(\"/content/validation_df.csv\")\n",
        "test_df.to_csv(\"/content/test_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftShqa9vfNR0"
      },
      "outputs": [],
      "source": [
        "# path_csv = sys.path[-1] + 'labels_crowdai.csv'\n",
        "# img_dir = 'object-detection-crowdai/'\n",
        "path_csv = \"/content/train_df.csv\"\n",
        "# img_dir = \"/content/trash-detection-35/train/\"\n",
        "img_dir = \"/content/Trash-Detection-14/train/\"\n",
        "\n",
        "path_csv_valid = \"/content/validation_df.csv\"\n",
        "# img_dir_test = \"/content/trash-detection-35/valid/\"\n",
        "img_dir_valid = \"/content/Trash-Detection-14/valid/\"\n",
        "\n",
        "path_csv_test = \"/content/test_df.csv\"\n",
        "img_dir_test = \"/content/Trash-Detection-14/test/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPcVqwjSfXDh"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(path_csv)\n",
        "print(\"Number of total bounding boxes: \", len(df))\n",
        "\n",
        "image_paths = glob.glob(f\"{img_dir}/*.jpg\")\n",
        "all_images = [image_path.split(os.path.sep)[-1] for image_path in image_paths]\n",
        "all_images = sorted(all_images)\n",
        "print(\"Number of unique images: \", len(all_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIoIEyEGUQ_Z"
      },
      "outputs": [],
      "source": [
        "df_valid = pd.read_csv(path_csv_valid)\n",
        "print(\"Number of total bounding boxes: \", len(df_valid))\n",
        "\n",
        "image_paths_valid = glob.glob(f\"{img_dir_valid}/*.jpg\")\n",
        "all_images_valid = [image_path_valid.split(os.path.sep)[-1] for image_path_valid in image_paths_valid]\n",
        "all_images_valid = sorted(all_images_valid)\n",
        "print(\"Number of unique images: \", len(all_images_valid))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(path_csv_test)\n",
        "print(\"Number of total bounding boxes: \", len(df_test))\n",
        "\n",
        "image_paths_test = glob.glob(f\"{img_dir_test}/*.jpg\")\n",
        "all_images_test = [image_path_test.split(os.path.sep)[-1] for image_path_test in image_paths_test]\n",
        "all_images_test = sorted(all_images_test)\n",
        "print(\"Number of unique images: \", len(all_images_test))"
      ],
      "metadata": {
        "id": "9jDm76oKd0kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7OPzLdTfZpX"
      },
      "source": [
        "## Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOFXZVaPfX1x"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Class that holds all the augmentation related attributes\n",
        "'''\n",
        "class Transformation():\n",
        "    # This provides a random probability of the augmentation to be applied or not\n",
        "    def get_probability(self):\n",
        "        return np.random.choice([False, True], replace=False, p=[0.5, 0.5])\n",
        "\n",
        "    # Increases the contrast by a factor of 2\n",
        "    def random_adjust_contrast(self, image, enable=None):\n",
        "        enable = self.get_probability() if enable is None else enable\n",
        "        return F.adjust_contrast(image, 2) if enable else image\n",
        "\n",
        "    # Increaes the brightness by a factor of 2\n",
        "    def random_adjust_brightness(self, image, enable=None):\n",
        "        enable = enable = self.get_probability() if enable is None else enable\n",
        "        return F.adjust_brightness(image,2) if enable else image\n",
        "\n",
        "    # Horizontal flip\n",
        "    def random_hflip(self, image, boxes, enable=None):\n",
        "        enable = enable = self.get_probability() if enable is None else enable\n",
        "        if enable:\n",
        "          #flip image\n",
        "          new_image = F.hflip(image)\n",
        "\n",
        "          #flip boxes\n",
        "          new_boxes = boxes.clone()\n",
        "          new_boxes[:, 0] = image.shape[2] - boxes[:, 0]  # image width - xmin\n",
        "          new_boxes[:, 2] = image.shape[2] - boxes[:, 2]  # image_width - xmax\n",
        "          new_boxes = new_boxes[:, [2, 1, 0, 3]]          # Interchange the xmin and xmax due to mirroring\n",
        "          return new_image, new_boxes\n",
        "        else:\n",
        "          return image, boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdihx0Wtfdfn"
      },
      "outputs": [],
      "source": [
        "class vehicleDataset(Dataset):\n",
        "  def __init__(self, img_path, label_path, classes, transforms=None):\n",
        "    super().__init__()\n",
        "    print(\"Preparing the dataset...\")\n",
        "\n",
        "    self.image_dir = img_path\n",
        "    self.gt_info = pd.read_csv(label_path)\n",
        "    self.classes = classes\n",
        "    self.transforms = transforms\n",
        "\n",
        "    # Create a list of image file names (in sorted order - this is optional)\n",
        "    self.image_paths = glob.glob(f\"{img_path}/*.jpg\")\n",
        "    all_images = [image_path.split(os.path.sep)[-1] for image_path in self.image_paths]\n",
        "    self.all_images = sorted(all_images)\n",
        "\n",
        "    images_to_keep = []\n",
        "\n",
        "    id_to_name = {id_: name for id_, name in enumerate(self.classes)}\n",
        "\n",
        "    # Map Label (str) --> Label (int)\n",
        "    for i in range(len(self.gt_info)):\n",
        "      label = self.gt_info.loc[i, 'category_id']\n",
        "      label = id_to_name[label]\n",
        "      if pd.notna(label):  # Check if label is not NaN\n",
        "              self.gt_info.loc[i, 'category_id'] = self.classes.index(label)\n",
        "              # Add the image name to the list of images to keep\n",
        "              images_to_keep.append(self.gt_info.loc[i, 'file_name'])\n",
        "\n",
        "    self.gt_info.loc[i, 'category_id'] = self.classes.index(label)\n",
        "\n",
        "    # Filter the dataset based on given conditions:\n",
        "    self.filter_dataset()\n",
        "    print(\"Dataset prepared\")\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    target = {}\n",
        "\n",
        "    # Read input image\n",
        "    # image_name = self.all_images[idx]\n",
        "    image_name = idx\n",
        "    image_path = os.path.join(self.image_dir, image_name)\n",
        "    image = cv2.imread(image_path)\n",
        "    image = img_transform(image)\n",
        "\n",
        "    # Fetch GT infos for given image\n",
        "    gt_info = self.gt_info[self.gt_info['file_name'] == image_name]\n",
        "\n",
        "    boxes = torch.Tensor(gt_info[['x1', 'y1', 'x2', 'y2']].values).float()\n",
        "    labels = torch.LongTensor(gt_info['category_id'].values.tolist())\n",
        "\n",
        "    if self.transforms:\n",
        "        # image = self.transforms.random_adjust_contrast(image, enable=True)\n",
        "        # image = self.transforms.random_adjust_brightness(image, enable=True)\n",
        "        # image, boxes = self.transforms.random_hflip(image, boxes, enable=True)\n",
        "        print(\"transforms\")\n",
        "\n",
        "    target[\"boxes\"] = boxes     # Hint: Shape -> [N, 4] with N = Number of Boxes\n",
        "    target[\"labels\"] = labels   # Hint: Shape -> [N] with N = Number of Boxes\n",
        "\n",
        "    return image, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.all_images)\n",
        "\n",
        "  def get_name(self, idx):\n",
        "    image_name = self.all_images[idx]\n",
        "    gt_info = self.gt_info[self.gt_info['file_name'] == image_name]\n",
        "    return gt_info[\"file_name\"]\n",
        "\n",
        "\n",
        "  '''\n",
        "  Filter the dataset by removing images with no labels and incorrect bounding boxes\n",
        "  '''\n",
        "  def filter_dataset(self):\n",
        "    print(\"Filtering the dataset...\")\n",
        "    remove_images = []\n",
        "\n",
        "    # There are no labels for some images because they show an 'empty' scene â†’ these images should be filtered out.\n",
        "    for image_file in self.all_images.copy():\n",
        "      if image_file not in self.gt_info['file_name'].values:\n",
        "        remove_images.append(image_file)\n",
        "        self.all_images.remove(image_file)\n",
        "    print(\"Images removed with no labels: \", len(remove_images))\n",
        "\n",
        "    # There are incorrect bounding boxes in the dataset (e.g. xmax=xmin).\n",
        "    valid_box_mask = (self.gt_info['x2'] - self.gt_info['x1'] > 0) & (self.gt_info['y2'] - self.gt_info['y1'] > 0)\n",
        "    print(\"Images removed with incorrect bounding boxes: \", len(self.gt_info) - valid_box_mask.shape[0])\n",
        "    self.gt_info = self.gt_info[valid_box_mask] # TODO: correct this masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70lS8bu_g6pu"
      },
      "outputs": [],
      "source": [
        "# labels_classes = [\"background\",\"Aluminium foil\",\"Bottle cap\",\"Broken glass\",\"Cigarette\",\"Clear plastic bottle\",\"Crisp packet\",\"Cup\",\n",
        "#           \"Drink can\",\"Food Carton\",\"Food container\",\"Food waste\",\"Garbage bag\",\"Glass bottle\",\"Lid\",\"Other Carton\",\"Other can\",\n",
        "#           \"Other container\",\"Other plastic\",\"Other plastic bottle\",\"Other plastic wrapper\",\"Paper\",\"Paper bag\",\"Plastic bag wrapper\",\n",
        "#           \"Plastic film\",\"Pop tab\",\"Single-use carrier bag\",\"Straw\",\"Styrofoam piece\",\"Unlabeled litter\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4SAFxSN2yDa"
      },
      "outputs": [],
      "source": [
        "labels_classes = [\"Background\",\"Glass\",\"Metal\",\"Paper\",\"Plastic\",\"Waste\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSB0K5IWpJtz"
      },
      "outputs": [],
      "source": [
        "id_to_name = {id_: name for id_, name in enumerate(labels_classes)}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = vehicleDataset(img_dir_test, path_csv_test, labels_classes, transforms=Transformation())\n",
        "num_images = 5\n",
        "# indexes = []\n",
        "# x = [\"glass207_jpg.rf.578f30def39b5c7d69d004f459e54e30.jpg\", \"000050_jpg.rf.043db995836b9fcb08a484206fe7a8c6.jpg\",\n",
        "#      \"paper132_jpg.rf.0008320c7b1e1857147763a80df50fe9.jpg\", \"000081_jpg.rf.0085f17f07c7ddf1f7b8790430fba450.jpg\",\n",
        "#      \"000055_jpg.rf.0c6f1c63f72987cc8c896a56123bf993.jpg\"]\n",
        "x = [\"glass252_jpg.rf.0588fd4e975a294f5062e7470e34dabe.jpg\", \"metal223_jpg.rf.0b435a8a304e807166bf2e4fa4c353ff.jpg\",\n",
        "     \"paper220_jpg.rf.57167f8cca6f68de931682ad0ef679bd.jpg\", \"PET1-648_jpg.rf.585745955900675e2976a0128a16a780.jpg\",\n",
        "     \"000007_jpg.rf.55298639ec0a0498f59e450e1ce69eb4.jpg\"]\n",
        "# for name in x:\n",
        "\n",
        "#   print(name, valid_df[valid_df['file_name'] == name]['ann_id'])\n",
        "\n",
        "# # x = [470, 5, 6, 0, 3, 40, 41, 42, 43, 44, 45]\n",
        "\n",
        "for i in range(len(x)):\n",
        "  img, target = dataset.__getitem__(x[i])\n",
        "  img = img.cpu().permute(1,2,0).numpy()\n",
        "  boxes = target['boxes'].numpy()\n",
        "  labels = target['labels'].numpy()\n",
        "  scores = [1]*len(labels)\n",
        "  # info = list(dataset.get_name(x[i]))\n",
        "  print(\"\\nName Image: \", x[i])\n",
        "  plot_image(img, boxes, scores, labels, labels_classes)"
      ],
      "metadata": {
        "id": "mXogLj0edLez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi8ykl92g7Uo"
      },
      "outputs": [],
      "source": [
        "dataset = vehicleDataset(img_dir, path_csv, labels_classes, transforms=Transformation()) # here your arguments\n",
        "num_images = 5\n",
        "\n",
        "for _ in range(num_images):\n",
        "  x = random.randint(0, (dataset.__len__()-1))\n",
        "  img, target = dataset.__getitem__(x)\n",
        "  img = img.cpu().permute(1,2,0).numpy()\n",
        "  boxes = target['boxes'].numpy()\n",
        "  labels = target['labels'].numpy()\n",
        "  scores = [1]*len(labels)\n",
        "  info = list(dataset.get_name(x))\n",
        "  print(\"\\nName Image: \", info[0])\n",
        "  plot_image(img, boxes, scores, labels, labels_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlmdBMV-hEmF"
      },
      "source": [
        "## Fine tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TILGUt-phBOa"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "LR = 0.005\n",
        "LR_MOMENTUM=0.9\n",
        "LR_DECAY_RATE=0.0005\n",
        "\n",
        "LR_SCHED_STEP_SIZE = 0.1\n",
        "LR_SCHED_GAMMA = 0.1\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "NUM_TEST_IMAGES = 5\n",
        "NMS_THRESH = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8awV8WIvhGAo"
      },
      "outputs": [],
      "source": [
        "current_dir = curren_dir if USE_COLAB else os.getcwd()\n",
        "\n",
        "# Fetch current date and time\n",
        "now = datetime.now()\n",
        "# dt_string = now.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
        "dt_string = \"images\"\n",
        "output_dir_name = \"output-\" + dt_string\n",
        "\n",
        "OUTPUT_DIR = os.path.join(current_dir, output_dir_name)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HYFNuYqhH-i"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(min_size=300, max_size=480, pretrained=True)\n",
        "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(min_size=300, max_size=480, weights=\"COCO_V1\").to(device)\n",
        "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1)\n",
        "\n",
        "num_classes = len(labels_classes)  # 3 classes + background\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = NUM_EPOCHS\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=LR, momentum=LR_MOMENTUM, weight_decay=LR_DECAY_RATE)\n",
        "\n",
        "# create a train- and validation-dataset with our vehicleDataset\n",
        "# split the dataset in train and test set\n",
        "# dataset = vehicleDataset(img_dir, path_csv, labels_classes, transforms=Transformation())      # here your arguments\n",
        "# dataset_test = vehicleDataset(img_dir, path_csv, labels_classes, transforms=None)             # here your arguments\n",
        "\n",
        "dataset = vehicleDataset(img_dir, path_csv, labels_classes, transforms=None)\n",
        "dataset_valid = vehicleDataset(img_dir_valid, path_csv_valid, labels_classes, transforms=None)\n",
        "dataset_test = vehicleDataset(img_dir_test, path_csv_test, labels_classes, transforms=None)\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "# indices = torch.randperm(len(dataset)).tolist()\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "indices_valid = torch.randperm(len(dataset_valid)).tolist()\n",
        "indices_test = torch.randperm(len(dataset_test)).tolist()\n",
        "\n",
        "#test_size = int(len(dataset) * TEST_SIZE)\n",
        "# dataset = torch.utils.data.Subset(dataset, indices[:-test_size])\n",
        "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-test_size:])\n",
        "dataset = torch.utils.data.Subset(dataset, indices)\n",
        "dataset_valid = torch.utils.data.Subset(dataset_valid, indices_valid)\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices_test)\n",
        "\n",
        "# create a learning rate scheduler\n",
        "# TODO: step size to be tuned !\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \\\n",
        "                                                step_size=LR_SCHED_STEP_SIZE, \\\n",
        "                                                gamma=LR_SCHED_GAMMA)\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return tuple(zip(*batch))\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "  dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,\n",
        "  collate_fn=collate_fn)\n",
        "\n",
        "# data_loader_valid = torch.utils.data.DataLoader(\n",
        "#   dataset_test, batch_size=1, shuffle=False, num_workers=2,\n",
        "#   collate_fn=collate_fn)\n",
        "\n",
        "data_loader_valid = torch.utils.data.DataLoader(\n",
        "  dataset_valid, batch_size=1, shuffle=False, num_workers=2,\n",
        "  collate_fn=collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size = 1, shuffle=False, num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TliNVsx1haYT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function to train the model over one epoch.\n",
        "'''\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "  train_loss_list = []\n",
        "\n",
        "  tqdm_bar = tqdm(data_loader, total=len(data_loader))\n",
        "  for idx, data in enumerate(tqdm_bar):\n",
        "    optimizer.zero_grad()\n",
        "    images, targets = data\n",
        "\n",
        "    images = list(image.to(device) for image in images)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # targets = {'boxes'=tensor, 'labels'=tensor}\n",
        "\n",
        "    losses = model(images, targets)\n",
        "\n",
        "    loss = sum(loss for loss in losses.values())\n",
        "    loss_val = loss.item()\n",
        "    train_loss_list.append(loss.detach().cpu().numpy())\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    tqdm_bar.set_description(desc=f\"Training Loss: {loss:.3f}\")\n",
        "\n",
        "  return train_loss_list\n",
        "\n",
        "'''\n",
        "Function to validate the model\n",
        "'''\n",
        "def evaluate(model, data_loader_test, device):\n",
        "    val_loss_list = []\n",
        "\n",
        "    tqdm_bar = tqdm(data_loader_test, total=len(data_loader_test))\n",
        "\n",
        "    for i, data in enumerate(tqdm_bar):\n",
        "        images, targets = data\n",
        "\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            losses = model(images, targets)\n",
        "\n",
        "        loss = sum(loss for loss in losses.values())\n",
        "        loss_val = loss.item()\n",
        "        val_loss_list.append(loss_val)\n",
        "\n",
        "        tqdm_bar.set_description(desc=f\"Validation Loss: {loss:.4f}\")\n",
        "    return val_loss_list\n",
        "\n",
        "'''\n",
        "Function to plot training and valdiation losses and save them in `output_dir'\n",
        "'''\n",
        "def plot_loss(train_loss, valid_loss):\n",
        "    figure_1, train_ax = plt.subplots()\n",
        "    figure_2, valid_ax = plt.subplots()\n",
        "\n",
        "    train_ax.plot(train_loss, color='blue')\n",
        "    train_ax.set_xlabel('Iteration')\n",
        "    train_ax.set_ylabel('Training Loss')\n",
        "\n",
        "    valid_ax.plot(valid_loss, color='red')\n",
        "    valid_ax.set_xlabel('Iteration')\n",
        "    valid_ax.set_ylabel('Validation loss')\n",
        "\n",
        "    figure_1.savefig(f\"{OUTPUT_DIR}/train_loss.png\")\n",
        "    figure_2.savefig(f\"{OUTPUT_DIR}/valid_loss.png\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3G0pW_VZhccn"
      },
      "outputs": [],
      "source": [
        "# find latest saved chcekpoint\n",
        "checkpoint_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.pth')]\n",
        "if checkpoint_files:\n",
        "    # Last ckpt file\n",
        "    checkpoint_files.sort()\n",
        "    latest_checkpoint = os.path.join(OUTPUT_DIR, checkpoint_files[-1])\n",
        "\n",
        "    # Load the ckpt\n",
        "    checkpoint = torch.load(latest_checkpoint)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    loss_dict = checkpoint['loss_dict']\n",
        "\n",
        "    print(\"Keep running from last check-point\")\n",
        "else:\n",
        "    start_epoch = 0\n",
        "    loss_dict = {'train_loss': [], 'valid_loss': []}\n",
        "\n",
        "'''\n",
        "Train the model over all epochs\n",
        "'''\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "  print(\"----------Epoch {}----------\".format(epoch+1))\n",
        "\n",
        "  # Train the model for one epoch\n",
        "  train_loss_list = train_one_epoch(model, optimizer, data_loader, device, epoch)\n",
        "  loss_dict['train_loss'].extend(train_loss_list)\n",
        "\n",
        "  lr_scheduler.step()\n",
        "\n",
        "  # Run evaluation\n",
        "  valid_loss_list = evaluate(model, data_loader_valid, device)\n",
        "  loss_dict['valid_loss'].extend(valid_loss_list)\n",
        "\n",
        "  # Svae the model ckpt after every epoch\n",
        "  ckpt_file_name = f\"{OUTPUT_DIR}/epoch_{epoch+1}_model.pth\"\n",
        "  torch.save({\n",
        "    'epoch': epoch+1,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss_dict': loss_dict\n",
        "  }, ckpt_file_name)\n",
        "\n",
        "  # NOTE: The losses are accumulated over all iterations\n",
        "  plot_loss(loss_dict['train_loss'], loss_dict['valid_loss'])\n",
        "\n",
        "# Store the losses after the training in a pickle\n",
        "with open(f\"{OUTPUT_DIR}/loss_dict.pkl\", \"wb\") as file:\n",
        "    pickle.dump(loss_dict, file)\n",
        "\n",
        "print(\"Training Finished !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M-9G8N_kIVPN"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "def results(img_name, class_name, confidence, boxes):\n",
        "  #<class_name> <confidence> <left> <top> <right> <bottom>\n",
        "  string = \"\"\n",
        "  path = \"/content/results/\"\n",
        "\n",
        "  if not os.path.exists(path):\n",
        "    print(\"Create directory\")\n",
        "    os.mkdir(path)\n",
        "\n",
        "  # create file with file_name\n",
        "  file_name = path + img_name[:-3] + \"txt\"\n",
        "\n",
        "  for i in range(len(class_name)):\n",
        "    # print(\"CLASS: \", class_name[i])\n",
        "    # print(\"CONFIDENCE: \", confidence[i])\n",
        "    # print(\"BOXES: \", str(boxes[i]))\n",
        "    x1 = str(boxes[i][0])\n",
        "    y1 = str(boxes[i][1])\n",
        "    x2 = str(boxes[i][2])\n",
        "    y2 = str(boxes[i][3])\n",
        "    # string = str(labels_classes[class_name[i] + 1]) + \" \" + str(confidence[i]) + str(boxes[i])\n",
        "    string += str(labels_classes[class_name[i]]) + \" \" + str(confidence[i]) + \" \" + x1 + \" \" + y1 + \" \" + x2 + \" \" + y2 + \"\\n\"\n",
        "\n",
        "  f = open(file_name, \"a\")\n",
        "  f.write(string)\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bw_xTjNTewQf"
      },
      "outputs": [],
      "source": [
        "def inference(img, img_name, model, detection_threshold=0.70):\n",
        "  '''\n",
        "  Infernece of a single input image\n",
        "\n",
        "  inputs:\n",
        "    img: input-image as torch.tensor (shape: [C, H, W])\n",
        "    model: model for infernce (torch.nn.Module)\n",
        "    detection_threshold: Confidence-threshold for NMS (default=0.7)\n",
        "\n",
        "  returns:\n",
        "    boxes: bounding boxes (Format [N, 4] => N times [xmin, ymin, xmax, ymax])\n",
        "    labels: class-prediction (Format [N] => N times an number between 0 and _num_classes-1)\n",
        "    scores: confidence-score (Format [N] => N times confidence-score between 0 and 1)\n",
        "  '''\n",
        "  model.eval()\n",
        "\n",
        "  img = img.to(device)\n",
        "  outputs = model([img])\n",
        "\n",
        "  boxes = outputs[0]['boxes'].data.cpu().numpy()\n",
        "  scores = outputs[0]['scores'].data.cpu().numpy()\n",
        "  labels = outputs[0]['labels'].data.cpu().numpy()\n",
        "\n",
        "  # print(\"Boxes: \", boxes)\n",
        "  # print(\"Labels: \", labels)\n",
        "  # print(\"Scores: \", scores)\n",
        "\n",
        "  results(img_name, labels, scores, boxes)\n",
        "\n",
        "  boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
        "  labels = labels[scores >= detection_threshold]\n",
        "  scores = scores[scores >= detection_threshold]\n",
        "\n",
        "  return boxes, scores, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9zx22PGUC0dg"
      },
      "outputs": [],
      "source": [
        "d = vehicleDataset(\"/content/Trash-Detection-14/test\", \"/content/test_df.csv\", labels_classes, transforms=Transformation()) # here your arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GmxsOEnsF59h"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/Trash-Detection-14/test/*.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uP15OQXchiC9"
      },
      "outputs": [],
      "source": [
        "random.seed(SEED)\n",
        "num_images = 110\n",
        "\n",
        "x = 0\n",
        "\n",
        "for filename in os.listdir(\"/content/Trash-Detection-14/test/\"):\n",
        "# for _ in range(num_images):\n",
        "  # x = random.randint(0, (d.__len__()-1))\n",
        "  print(\"FILE NAME: \", filename)\n",
        "  img, target = d.__getitem__(x)\n",
        "  img = img.to(device)\n",
        "  info = list(d.get_name(x))\n",
        "  print(\"File name: \", info[0])\n",
        "\n",
        "  # name_image = list(dataset_test.get_name(x))\n",
        "  # print(\"Image: \", name_image)\n",
        "\n",
        "  # Load last checkpoint\n",
        "  # CHANGE THE OUTPUT_DIR IF CKPT IS STORED ELSEWHERE\n",
        "  checkpoint_dir = f\"/{OUTPUT_DIR}/epoch_5_model.pth\"\n",
        "  checkpoint = torch.load(checkpoint_dir, map_location=device)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "  boxes, scores, labels = inference(img, info[0], model)\n",
        "\n",
        "  img = img.squeeze(0).cpu().permute(1,2,0).numpy()\n",
        "  plot_image(img, boxes, scores, labels, labels_classes, save_path=f\"{OUTPUT_DIR}/\" + info[0])\n",
        "\n",
        "  x += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05abOjBEO2wF"
      },
      "source": [
        "prime 15 esecuzioni<br>\n",
        "Training Loss: 0.134<br>\n",
        "Validation Loss: 0.0716<br>\n",
        "parameters: LR = 0.005\n",
        "LR_MOMENTUM=0.9\n",
        "LR_DECAY_RATE=0.0005\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kDFy87XiFid"
      },
      "source": [
        "Training Loss: 0.022:\n",
        "Validation Loss: 0.0506:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5VjdlMYsnAa0"
      },
      "outputs": [],
      "source": [
        "#!zip -r output_resnet.zip /content/output-12-04-2024-07-08-06/*.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OdeNRer5naDA"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.download(\"/content/output_resnet.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bQY3wJFUsVY8"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/results_images/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GVMGYAD6U09i"
      },
      "outputs": [],
      "source": [
        "%cd /content/output-images\n",
        "%cp *.jpg /content/results_images/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cm-XJVCIV6An"
      },
      "outputs": [],
      "source": [
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CyVZ8P4zSmAH"
      },
      "outputs": [],
      "source": [
        "!zip -r results.zip /content/results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9fYD7R05t8oC"
      },
      "outputs": [],
      "source": [
        "!zip -r results_images.zip /content/results_images/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jcql26Y8SwC4"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/results.zip\")\n",
        "files.download(\"/content/results_images.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}